import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
import base64
import io
import os
import uuid
import tempfile
import logging

from flask import Flask, request, jsonify, send_file
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
import shap

from werkzeug.utils import secure_filename

# ========== è®¾ç½®æ—¥å¿— ==========
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)

# ========== å…¨å±€å˜é‡ï¼šåŠ è½½æ¨¡å‹å’Œå…ƒæ•°æ® ==========
MODEL_DIR = 'output'

required_files = [
    'gbdt_model.pkl',
    'lr_model.pkl',
    'train_feature_names.csv',
    'category_features.csv',
    'continuous_features.csv',
    'actual_n_estimators.csv'  # ğŸ†• æ–°å¢ï¼šç¡®ä¿æ ‘æ•°é‡ä¸€è‡´
]

for f in required_files:
    if not os.path.exists(os.path.join(MODEL_DIR, f)):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°å¿…éœ€æ–‡ä»¶: {f}ã€‚è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹ã€‚")

gbdt_model = joblib.load(os.path.join(MODEL_DIR, 'gbdt_model.pkl'))
lr_model = joblib.load(os.path.join(MODEL_DIR, 'lr_model.pkl'))

train_feature_names = pd.read_csv(os.path.join(MODEL_DIR, 'train_feature_names.csv'))['feature'].tolist()
category_features = pd.read_csv(os.path.join(MODEL_DIR, 'category_features.csv'))['feature'].tolist()
continuous_features = pd.read_csv(os.path.join(MODEL_DIR, 'continuous_features.csv'))['feature'].tolist()

# ğŸ†• åŠ è½½å®é™…æ ‘æ•°é‡
actual_n_estimators = pd.read_csv(os.path.join(MODEL_DIR, 'actual_n_estimators.csv'))['n_estimators'].iloc[0]
print(f"âœ… å®é™…æ ‘æ•°é‡: {actual_n_estimators}")

category_prefixes = [col + "_" for col in category_features]

print("âœ… æ¨¡å‹å’Œå…ƒæ•°æ®åŠ è½½å®Œæˆ")


# ========== å·¥å…·å‡½æ•°ï¼šè§£æå¶å­èŠ‚ç‚¹è·¯å¾„ ==========
def get_leaf_path_enhanced(booster, tree_index, leaf_index, feature_names, category_prefixes):
    try:
        model_dump = booster.dump_model()
        if tree_index >= len(model_dump['tree_info']):
            return None
        tree_info = model_dump['tree_info'][tree_index]['tree_structure']
    except Exception as e:
        print(f"è§£ææ ‘ç»“æ„å¤±è´¥: {e}")
        return None

    node_stack = [(tree_info, [])]

    while node_stack:
        node, current_path = node_stack.pop()

        if 'leaf_index' in node and node['leaf_index'] == leaf_index:
            return current_path

        if 'split_feature' in node:
            feat_idx = node['split_feature']
            if feat_idx >= len(feature_names):
                feat_name = f"Feature_{feat_idx}"
            else:
                feat_name = feature_names[feat_idx]

            threshold = node.get('threshold', 0.0)
            decision_type = node.get('decision_type', '<=')

            is_category = False
            original_col = None
            category_value = None

            for prefix in category_prefixes:
                if feat_name.startswith(prefix):
                    is_category = True
                    original_col = prefix.rstrip('_')
                    category_value = feat_name[len(prefix):]
                    break

            if is_category:
                right_rule = f"{original_col} == '{category_value}'"
                left_rule = f"{original_col} != '{category_value}'"
            else:
                if decision_type == '<=' or decision_type == 'no_greater':
                    right_rule = f"{feat_name} > {threshold:.4f}"
                    left_rule = f"{feat_name} <= {threshold:.4f}"
                else:
                    right_rule = f"{feat_name} {decision_type} {threshold:.4f}"
                    left_rule = f"{feat_name} not {decision_type} {threshold:.4f}"

            if 'right_child' in node:
                node_stack.append((node['right_child'], current_path + [right_rule]))
            if 'left_child' in node:
                node_stack.append((node['left_child'], current_path + [left_rule]))

    return None


# ========== å·¥å…·å‡½æ•°ï¼šé¢„å¤„ç†å•æ ·æœ¬ ==========
def preprocess_single_sample(sample_dict):
    sample_df = pd.DataFrame([sample_dict])

    for col in continuous_features:
        if col not in sample_df.columns:
            sample_df[col] = -1
        else:
            sample_df[col] = pd.to_numeric(sample_df[col], errors='coerce').fillna(-1)

    all_dummies_list = []

    for col in category_features:
        if col in sample_df.columns:
            val = sample_df[col].iloc[0]
            if pd.isna(val) or val == "":
                sample_df[col] = "-1"
            else:
                sample_df[col] = str(val)
        else:
            sample_df[col] = "-1"

        sample_df[col] = sample_df[col].astype('category')
        dummies = pd.get_dummies(sample_df[col], prefix=col)

        missing_cols = [
            train_col for train_col in train_feature_names
            if train_col.startswith(col + "_") and train_col not in dummies.columns
        ]
        if missing_cols:
            missing_df = pd.DataFrame(0, index=dummies.index, columns=missing_cols)
            dummies = pd.concat([dummies, missing_df], axis=1)

        all_dummies_list.append(dummies)

    if all_dummies_list:
        dummies_combined = pd.concat(all_dummies_list, axis=1)
        sample_df = pd.concat([sample_df.drop(columns=category_features), dummies_combined], axis=1)
    else:
        sample_df = sample_df.drop(columns=category_features)

    missing_final_cols = set(train_feature_names) - set(sample_df.columns)
    if missing_final_cols:
        missing_final_df = pd.DataFrame(0, index=sample_df.index, columns=list(missing_final_cols))
        sample_df = pd.concat([sample_df, missing_final_df], axis=1)

    sample_df = sample_df.reindex(columns=train_feature_names, fill_value=0)
    return sample_df


# ========== æ ¸å¿ƒé¢„æµ‹å‡½æ•°ï¼ˆä¾› /predict å’Œ /predict_batch_csv å…±ç”¨ï¼‰ ==========
def predict_core(sample_df_list, return_explanation=True, generate_plot=False):
    """
    å¯¹ä¸€ç»„æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼ˆå¯æ‰¹é‡ï¼‰
    :param sample_df_list: List of DataFrames, each with 1 row (preprocessed)
    :param return_explanation: æ˜¯å¦è¿”å›ç‰¹å¾/è§„åˆ™è§£é‡Š
    :param generate_plot: æ˜¯å¦ç”Ÿæˆ SHAP å›¾ï¼ˆä»…ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰¹é‡æ—¶ä¸å»ºè®®ï¼‰
    :return: List of result dicts
    """
    if not sample_df_list:
        return []

    # åˆå¹¶ä¸ºå¤§ DataFrame
    batch_df = pd.concat(sample_df_list, ignore_index=True)

    # Step 1: GBDT å¶å­ç´¢å¼• â€”â€” ğŸ†• ä½¿ç”¨ actual_n_estimators
    leaf_indices_batch = gbdt_model.booster_.predict(batch_df.values, pred_leaf=True)
    n_trees = actual_n_estimators  # âœ… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å®é™…æ ‘æ•°é‡

    # Step 2: å¶å­ One-Hot
    leaf_dummies_list = []
    for i in range(n_trees):
        leaf_col_name = f"gbdt_leaf_{i}"
        leaf_series = pd.Series(leaf_indices_batch[:, i], name=leaf_col_name)
        dummies = pd.get_dummies(leaf_series, prefix=leaf_col_name)
        leaf_dummies_list.append(dummies)

    leaf_dummies_combined = pd.concat(leaf_dummies_list, axis=1) if leaf_dummies_list else pd.DataFrame()

    lr_feature_names = getattr(lr_model, 'feature_names_in_', [f"feature_{i}" for i in range(len(lr_model.coef_[0]))])
    missing_leaf_cols = set(lr_feature_names) - set(leaf_dummies_combined.columns)
    if missing_leaf_cols:
        missing_leaf_df = pd.DataFrame(0, index=leaf_dummies_combined.index, columns=list(missing_leaf_cols))
        leaf_dummies_combined = pd.concat([leaf_dummies_combined, missing_leaf_df], axis=1)

    leaf_dummies_combined = leaf_dummies_combined.reindex(columns=lr_feature_names, fill_value=0)

    # Step 3: LR æ¦‚ç‡
    probabilities = lr_model.predict_proba(leaf_dummies_combined)[:, 1]

    results = []

    if not return_explanation:
        for i in range(len(sample_df_list)):
            results.append({
                "probability": round(float(probabilities[i]), 4),
                "explanation": None
            })
        return results

    # Step 4: æ‰¹é‡ SHAPï¼ˆåªç®—ä¸€æ¬¡ï¼‰
    explainer = shap.TreeExplainer(gbdt_model.booster_)
    shap_values_batch = explainer.shap_values(batch_df)
    if isinstance(shap_values_batch, list):
        shap_values_batch = shap_values_batch[1]  # æ­£ç±»

    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆè§£é‡Š
    for idx in range(len(sample_df_list)):
        shap_vals = shap_values_batch[idx]
        feature_imp = [(train_feature_names[i], float(shap_vals[i])) for i in range(len(shap_vals))]
        feature_imp.sort(key=lambda x: abs(x[1]), reverse=True)

        important_features = [
            {"feature": feat, "shap_value": round(val, 4)}
            for feat, val in feature_imp[:3]
        ]

        top_shap_features = [feat for feat, val in feature_imp[:5]]
        leaf_indices = leaf_indices_batch[idx]

        # åŸå§‹è·¯å¾„è§„åˆ™
        path_rules = []
        for tree_idx in range(min(3, len(leaf_indices))):
            leaf_idx = leaf_indices[tree_idx]
            rule = get_leaf_path_enhanced(
                gbdt_model.booster_,
                tree_index=tree_idx,
                leaf_index=leaf_idx,
                feature_names=train_feature_names,
                category_prefixes=category_prefixes
            )
            if rule:
                path_rules.extend(rule[:3])

        seen = set()
        unique_path_rules = []
        for r in path_rules:
            if r not in seen:
                seen.add(r)
                unique_path_rules.append(r)
        top_rules = unique_path_rules[:5]

        # ç‰¹å¾å…³è”è§„åˆ™
        feature_rules = []
        for tree_idx in range(min(10, len(leaf_indices))):
            leaf_idx = leaf_indices[tree_idx]
            rule = get_leaf_path_enhanced(
                gbdt_model.booster_,
                tree_index=tree_idx,
                leaf_index=leaf_idx,
                feature_names=train_feature_names,
                category_prefixes=category_prefixes
            )
            if rule:
                for r in rule:
                    for feat in top_shap_features:
                        if feat in r or (feat.split('_')[0] + " " in r) or (feat.split('_')[0] + " ==" in r):
                            shap_val = next((val for f, val in feature_imp if f == feat), 0)
                            rule_with_shap = f"{r} (SHAP: {shap_val:+.4f})"
                            if rule_with_shap not in feature_rules:
                                feature_rules.append(rule_with_shap)
                            break
                    if len(feature_rules) >= 5:
                        break
            if len(feature_rules) >= 5:
                break

        # ç”Ÿæˆå›¾ï¼ˆä»…ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œä¸”ä»…å½“ generate_plot=Trueï¼‰
        shap_plot_b64 = ""
        if generate_plot and idx == 0:
            try:
                plt.figure(figsize=(10, 6))
                shap.waterfall_plot(
                    shap.Explanation(
                        values=shap_vals,
                        base_values=explainer.expected_value,
                        data=batch_df.iloc[idx],
                        feature_names=batch_df.columns.tolist()
                    ),
                    show=False
                )
                plt.title("SHAP Explanation for Prediction", fontsize=14)
                plt.tight_layout()

                buf = io.BytesIO()
                plt.savefig(buf, format='png', dpi=150, bbox_inches='tight')
                plt.close()
                buf.seek(0)
                shap_plot_b64 = "image/png;base64," + base64.b64encode(buf.read()).decode('utf-8')
                buf.close()
            except Exception as e:
                print("âš ï¸ SHAP plot generation failed:", e)

        explanation = {
            "important_features": important_features,
            "shap_plot_base64": shap_plot_b64,
            "top_rules": top_rules,
            "feature_based_rules": feature_rules[:5]
        }

        results.append({
            "probability": round(float(probabilities[idx]), 4),
            "explanation": explanation
        })

    return results


# ========== API è·¯ç”±ï¼šå•æ ·æœ¬é¢„æµ‹ ==========
@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No JSON data provided"}), 400
        if not isinstance(data, dict):
            return jsonify({"error": "JSON body must be an object"}), 400

        sample_df = preprocess_single_sample(data)
        results = predict_core([sample_df], return_explanation=True, generate_plot=True)

        if not results:
            return jsonify({"error": "Prediction failed"}), 500

        return jsonify(results[0])  # å•æ ·æœ¬ï¼Œå–ç¬¬ä¸€ä¸ª

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# ========== API è·¯ç”±ï¼šæ‰¹é‡é¢„æµ‹ï¼ˆä¸Šä¼  CSVï¼‰==========
@app.route('/predict_batch_csv', methods=['POST'])
def predict_batch_csv():
    """
    æ‰¹é‡é¢„æµ‹æ¥å£ï¼šæ¥æ”¶ä¸Šä¼ çš„ CSV æ–‡ä»¶ï¼Œè¿”å›å¸¦é¢„æµ‹ç»“æœçš„ CSV
    """
    try:
        if 'file' not in request.files:
            return jsonify({"error": "No file part in request"}), 400

        file = request.files['file']
        if file.filename == '':
            return jsonify({"error": "No selected file"}), 400

        if not file.filename.endswith('.csv'):
            return jsonify({"error": "Only CSV files are allowed"}), 400

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ å†…å®¹
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp:
            file.save(tmp.name)
            tmp_path = tmp.name

        # è¯»å–ä¸Šä¼ çš„ CSV
        input_df = pd.read_csv(tmp_path)
        os.unlink(tmp_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶

        if input_df.empty:
            return jsonify({"error": "Uploaded CSV is empty"}), 400

        print(f"ğŸ“¥ æ”¶åˆ° {len(input_df)} è¡Œæ•°æ®è¿›è¡Œæ‰¹é‡é¢„æµ‹")

        # é¢„å¤„ç†æ‰€æœ‰æ ·æœ¬
        processed_rows = []
        for idx, row in input_df.iterrows():
            sample_dict = row.to_dict()
            processed_sample = preprocess_single_sample(sample_dict)
            processed_rows.append(processed_sample)

        # æ ¸å¿ƒé¢„æµ‹ï¼ˆä¸ç”Ÿæˆå›¾ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        results = predict_core(processed_rows, return_explanation=True, generate_plot=False)

        # æ„é€ ç®€æ´ç»“æœç”¨äº CSV
        csv_results = []
        for r in results:
            exp = r["explanation"]
            top_features = "; ".join([
                f"{feat['feature']}({feat['shap_value']:+.3f})"
                for feat in exp["important_features"]
            ]) if exp else ""
            top_rules = "; ".join(exp["top_rules"][:3]) if exp else ""

            csv_results.append({
                "probability": r["probability"],
                "top_features": top_features,
                "top_rules": top_rules
            })

        # ========== ç”Ÿæˆç»“æœ DataFrame ==========
        # è‡ªåŠ¨è¯†åˆ«åŸå§‹ç¬¬ä¸€åˆ—ä½œä¸º ID åˆ—
        original_id_col_name = input_df.columns[0]
        original_id_series = input_df.iloc[:, 0].copy()

        result_df = pd.DataFrame()
        result_df['prediction_probability'] = [r['probability'] for r in csv_results]  # ç¬¬ä¸€åˆ—ï¼šé¢„æµ‹æ¦‚ç‡
        result_df[original_id_col_name] = original_id_series.values  # ç¬¬äºŒåˆ—ï¼šåŸå§‹ID

        # æ’å…¥å…¶ä½™åŸå§‹åˆ—ï¼ˆä»ç¬¬2åˆ—å¼€å§‹ï¼‰
        for col in input_df.columns[1:]:
            result_df[col] = input_df[col].values

        # æœ€åè¿½åŠ è§£é‡Šåˆ—
        result_df['top_3_features'] = [r['top_features'] for r in csv_results]
        result_df['top_3_rules'] = [r['top_rules'] for r in csv_results]

        # ========== è¾“å‡º CSV ==========
        output_filename = f"predictions_{uuid.uuid4().hex[:8]}.csv"
        output_path = os.path.join(tempfile.gettempdir(), output_filename)

        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')  # utf-8-sig æ”¯æŒä¸­æ–‡

        response = send_file(
            output_path,
            mimetype='text/csv',
            as_attachment=True,
            download_name=output_filename
        )

        # è¯·æ±‚ç»“æŸååˆ é™¤ä¸´æ—¶æ–‡ä»¶
        @response.call_on_close
        def remove_file():
            try:
                os.remove(output_path)
                logger.info(f"âœ… ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {output_path}")
            except Exception as e:
                logger.error(f"âŒ ä¸´æ—¶æ–‡ä»¶åˆ é™¤å¤±è´¥: {e}")

        return response

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {str(e)}"}), 500


# ========== å¥åº·æ£€æŸ¥ ==========
@app.route('/health', methods=['GET'])
def health():
    return jsonify({
        "status": "healthy",
        "message": "GBDT+LR API is running",
        "model_loaded": True,
        "features": {
            "continuous": len(continuous_features),
            "categorical": len(category_features),
            "total_input_features": len(train_feature_names)
        },
        "model_info": {
            "actual_n_estimators": int(actual_n_estimators)  # ğŸ†• æ–°å¢
        }
    })


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨ Flask API æœåŠ¡...")
    print("   è®¿é—®å¥åº·æ£€æŸ¥: http://localhost:5000/health")
    print("   å•æ ·æœ¬é¢„æµ‹: POST http://localhost:5000/predict")
    print("   æ‰¹é‡é¢„æµ‹CSV: POST http://localhost:5000/predict_batch_csv (ä¸Šä¼  file)")
    app.run(host='0.0.0.0', port=5000, debug=True)
