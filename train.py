import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
from lightgbm import log_evaluation
import matplotlib.pyplot as plt
import os
import joblib

# ========== å·¥å…·å‡½æ•°ï¼šè§£æå¶å­èŠ‚ç‚¹è·¯å¾„ï¼ˆå¢å¼ºç‰ˆï¼‰ ==========
def get_leaf_path_enhanced(booster, tree_index, leaf_index, feature_names, category_prefixes):
    """
    è§£ææŒ‡å®šå¶å­èŠ‚ç‚¹çš„å†³ç­–è·¯å¾„ï¼Œæ”¯æŒç¿»è¯‘ one-hot ç±»åˆ«ç‰¹å¾
    """
    try:
        model_dump = booster.dump_model()
        if tree_index >= len(model_dump['tree_info']):
            return None
        tree_info = model_dump['tree_info'][tree_index]['tree_structure']
    except Exception as e:
        print(f"è·å–æ ‘ç»“æ„å¤±è´¥: {e}")
        return None

    node_stack = [(tree_info, [])]  # (å½“å‰èŠ‚ç‚¹, è·¯å¾„åˆ—è¡¨)

    while node_stack:
        node, current_path = node_stack.pop()

        # å¦‚æœæ˜¯ç›®æ ‡å¶å­èŠ‚ç‚¹
        if 'leaf_index' in node and node['leaf_index'] == leaf_index:
            return current_path

        # å¦‚æœæ˜¯åˆ†è£‚èŠ‚ç‚¹
        if 'split_feature' in node:
            feat_idx = node['split_feature']
            if feat_idx >= len(feature_names):
                feat_name = f"Feature_{feat_idx}"
            else:
                feat_name = feature_names[feat_idx]

            threshold = node.get('threshold', 0.0)
            decision_type = node.get('decision_type', '<=')

            # æ£€æŸ¥æ˜¯å¦ä¸º one-hot ç±»åˆ«ç‰¹å¾
            is_category = False
            original_col = None
            category_value = None

            for prefix in category_prefixes:
                if feat_name.startswith(prefix):
                    is_category = True
                    original_col = prefix.rstrip('_')
                    category_value = feat_name[len(prefix):]
                    break

            if is_category:
                # ç±»åˆ«ç‰¹å¾é€šå¸¸ç”¨ > 0.5 åˆ¤æ–­æ˜¯å¦æ¿€æ´»
                # å‡è®¾å³å­æ ‘æ˜¯â€œç­‰äºè¯¥ç±»åˆ«â€
                right_rule = f"{original_col} == '{category_value}'"
                left_rule = f"{original_col} != '{category_value}'"
            else:
                # è¿ç»­ç‰¹å¾
                if decision_type == '<=' or decision_type == 'no_greater':
                    right_rule = f"{feat_name} > {threshold:.4f}"
                    left_rule = f"{feat_name} <= {threshold:.4f}"
                else:
                    right_rule = f"{feat_name} {decision_type} {threshold:.4f}"
                    left_rule = f"{feat_name} not {decision_type} {threshold:.4f}"

            # æ·»åŠ å·¦å³å­æ ‘åˆ°æ ˆ
            if 'right_child' in node:
                node_stack.append((node['right_child'], current_path + [right_rule]))
            if 'left_child' in node:
                node_stack.append((node['left_child'], current_path + [left_rule]))

    return None  # æœªæ‰¾åˆ°è·¯å¾„


# ========== æ•°æ®é¢„å¤„ç† ==========
def preProcess():
    path = 'data/'
    try:
        df_train = pd.read_csv(path + 'train.csv', encoding='utf-8')
        df_test = pd.read_csv(path + 'test.csv', encoding='utf-8')
    except UnicodeDecodeError:
        print("âš ï¸ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ GBK ç¼–ç ...")
        df_train = pd.read_csv(path + 'train.csv', encoding='gbk')
        df_test = pd.read_csv(path + 'test.csv', encoding='gbk')
    
    test_ids = df_test['Id'].copy()
    df_train.drop(['Id'], axis=1, inplace=True)
    df_test.drop(['Id'], axis=1, inplace=True)
    
    df_test['Label'] = -1
    data = pd.concat([df_train, df_test], ignore_index=True)
    data = data.fillna(-1)
    
    data.to_csv('data/data.csv', index=False, encoding='utf-8')
    return data, test_ids


# ========== GBDT + LR æ ¸å¿ƒè®­ç»ƒé¢„æµ‹å‡½æ•° ==========
def gbdt_lr_predict(data, category_feature, continuous_feature, test_ids):
    """
    ä½¿ç”¨ GBDT + LRï¼Œå¢å¼ºå¯è§£é‡Šæ€§è¾“å‡º
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # ========== Step 1: ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç  ==========
    for col in category_feature:
        if data[col].dtype == 'object':
            data[col] = data[col].astype('category')
        onehot_feats = pd.get_dummies(data[col], prefix=col)
        data.drop([col], axis=1, inplace=True)
        data = pd.concat([data, onehot_feats], axis=1)

    train = data[data['Label'] != -1].copy()
    target = train.pop('Label')
    test = data[data['Label'] == -1].copy()
    test.drop(['Label'], axis=1, inplace=True)

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    x_train, x_val, y_train, y_val = train_test_split(
        train, target, test_size=0.2, random_state=2020, stratify=target
    )

    # ========== Step 2: è®­ç»ƒ GBDT ==========
    n_estimators = 32
    num_leaves = 64

    model = lgb.LGBMClassifier(
        objective='binary',
        boosting_type='gbdt',
        subsample=0.8,
        min_child_weight=0.1,
        min_child_samples=10,
        colsample_bytree=0.7,
        num_leaves=num_leaves,
        learning_rate=0.05,
        n_estimators=n_estimators,
        random_state=2020,
        n_jobs=-1
    )

    model.fit(
        x_train, y_train,
        eval_set=[(x_train, y_train), (x_val, y_val)],
        eval_metric='binary_logloss',
        callbacks=[
            log_evaluation(0),
            lgb.early_stopping(stopping_rounds=5, verbose=False)
        ]
    )

    # ========== ğŸ†• è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡ ==========
    actual_n_estimators = model.best_iteration_
    print(f"âœ… å®é™…è®­ç»ƒæ ‘æ•°é‡: {actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")

    # ========== Step 2.5: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ ==========
    feat_imp = pd.DataFrame({
        'Feature': x_train.columns,
        'Importance': model.feature_importances_
    }).sort_values('Importance', ascending=False)

    print("\n" + "="*60)
    print("ğŸ“Š GBDT Top 20 é‡è¦ç‰¹å¾:")
    print("="*60)
    print(feat_imp.head(20))
    feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False)
    print("âœ… å·²ä¿å­˜è‡³ output/gbdt_feature_importance.csv")

    # ========== Step 3: è·å–å¶å­èŠ‚ç‚¹ç´¢å¼• ==========
    gbdt_feats_train = model.booster_.predict(train.values, pred_leaf=True)
    gbdt_feats_test = model.booster_.predict(test.values, pred_leaf=True)

    print("\nâœ… GBDT å¶å­èŠ‚ç‚¹ç´¢å¼• shape:", gbdt_feats_train.shape)
    print("âœ… å‰5ä¸ªæ ·æœ¬å¶å­ç´¢å¼•:\n", gbdt_feats_train[:5])

    # ========== Step 3.5: ç¤ºä¾‹æ ·æœ¬å¶å­è·¯å¾„ + è§„åˆ™è§£æ ==========
    print("\n" + "="*70)
    print("ğŸ” è§£æå¶å­èŠ‚ç‚¹ç¤ºä¾‹ï¼šgbdt_leaf_5_22")
    print("="*70)
    
    try:
        # ç”Ÿæˆç±»åˆ«å‰ç¼€åˆ—è¡¨ï¼ˆç”¨äºè¯†åˆ« one-hot åˆ—ï¼‰
        category_prefixes = [col + "_" for col in category_feature]
        
        # è§£æç¬¬5æ£µæ ‘ã€ç¬¬22å·å¶å­
        leaf_rule = get_leaf_path_enhanced(
            model.booster_, 
            tree_index=5, 
            leaf_index=22, 
            feature_names=x_train.columns.tolist(),
            category_prefixes=category_prefixes
        )
        
        if leaf_rule:
            print(f"âœ… å¶å­èŠ‚ç‚¹ gbdt_leaf_5_22 çš„å†³ç­–è·¯å¾„ï¼š")
            for i, rule in enumerate(leaf_rule, 1):
                print(f"   {i}. {rule}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°è¯¥å¶å­èŠ‚ç‚¹è·¯å¾„ï¼ˆå¯èƒ½ç´¢å¼•è¶Šç•Œæˆ–æ ‘ç»“æ„å˜åŒ–ï¼‰")
            
    except Exception as e:
        print("âš ï¸ è§£æå¤±è´¥:", e)

    # ========== Step 4: å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç  ==========
    # ğŸ†• ä½¿ç”¨ actual_n_estimators æ›¿ä»£ç¡¬ç¼–ç  n_estimators
    gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(actual_n_estimators)]

    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)
    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns=gbdt_feats_name)

    train_len = df_train_gbdt_feats.shape[0]
    data_gbdt = pd.concat([df_train_gbdt_feats, df_test_gbdt_feats], ignore_index=True)

    for col in gbdt_feats_name:
        onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)
        data_gbdt.drop([col], axis=1, inplace=True)
        data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)

    train_lr = data_gbdt.iloc[:train_len, :].reset_index(drop=True)
    test_lr = data_gbdt.iloc[train_len:, :].reset_index(drop=True)

    # ========== Step 5: è®­ç»ƒ LR æ¨¡å‹ ==========
    x_train_lr, x_val_lr, y_train_lr, y_val_lr = train_test_split(
        train_lr, target, test_size=0.3, random_state=2018, stratify=target
    )

    lr = LogisticRegression(
        penalty='l2',
        C=0.1,
        solver='liblinear',
        random_state=2018,
        max_iter=1000
    )
    lr.fit(x_train_lr, y_train_lr)

    tr_logloss = log_loss(y_train_lr, lr.predict_proba(x_train_lr)[:, 1])
    val_logloss = log_loss(y_val_lr, lr.predict_proba(x_val_lr)[:, 1])
    print('\nâœ… Train LogLoss:', tr_logloss)
    print('âœ… Val LogLoss:', val_logloss)

    # ========== Step 5.5: è¾“å‡º LR ç³»æ•°ï¼ˆå“ªäº›å¶å­è§„åˆ™æœ€é‡è¦ï¼‰ ==========
    lr_coef = pd.DataFrame({
        'Leaf_Feature': x_train_lr.columns,
        'Coefficient': lr.coef_[0]
    }).sort_values('Coefficient', key=abs, ascending=False)

    print("\n" + "="*60)
    print("ğŸ“Š LR Top 20 é‡è¦å¶å­ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºï¼‰:")
    print("="*60)
    print(lr_coef.head(20))
    lr_coef.to_csv('output/lr_leaf_coefficients.csv', index=False)
    print("âœ… å·²ä¿å­˜è‡³ output/lr_leaf_coefficients.csv")

    # ========== Step 5.6: å¯¹é«˜æƒé‡å¶å­è¿›è¡Œè§„åˆ™è§£æ ==========
    print("\n" + "="*70)
    print("ğŸ§  è§£æ LR ä¸­é«˜æƒé‡å¶å­èŠ‚ç‚¹å¯¹åº”çš„åŸå§‹è§„åˆ™")
    print("="*70)
    
    top_leaves = lr_coef.head(5)  # è§£æå‰5ä¸ªæœ€é‡è¦å¶å­
    category_prefixes = [col + "_" for col in category_feature]
    
    for idx, row in top_leaves.iterrows():
        leaf_feat = row['Leaf_Feature']
        coef = row['Coefficient']
        
        # è§£æå¶å­åç§°ï¼Œå¦‚ "gbdt_leaf_5_22"
        if leaf_feat.startswith('gbdt_leaf_'):
            parts = leaf_feat.split('_')
            if len(parts) >= 4:
                tree_idx = int(parts[2])
                leaf_idx = int(parts[3])
                
                print(f"\nğŸ” è§£æ {leaf_feat} (LRç³»æ•°: {coef:.4f})")
                try:
                    rule = get_leaf_path_enhanced(
                        model.booster_,
                        tree_index=tree_idx,
                        leaf_index=leaf_idx,
                        feature_names=x_train.columns.tolist(),
                        category_prefixes=category_prefixes
                    )
                    if rule:
                        for i, r in enumerate(rule, 1):
                            print(f"   {i}. {r}")
                    else:
                        print("   âš ï¸ è·¯å¾„æœªæ‰¾åˆ°")
                except Exception as e:
                    print(f"   âš ï¸ è§£æå¤±è´¥: {e}")

    # ========== Step 6: SHAP è§£é‡Šï¼ˆå…¨å±€ + å•æ ·æœ¬ï¼‰ ==========
    print("\n" + "="*60)
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆ SHAP å¯è§£é‡Šæ€§å›¾è¡¨...")
    print("="*60)
    
    try:
        import shap
        explainer = shap.TreeExplainer(model.booster_)
        
        sample_size = min(100, len(x_val))
        x_val_sample = x_val.iloc[:sample_size]
        shap_values = explainer.shap_values(x_val_sample)

        # 1. å…¨å±€ç‰¹å¾é‡è¦æ€§å›¾
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, x_val_sample, feature_names=x_val_sample.columns.tolist(), show=False)
        plt.title("SHAP Feature Importance (GBDT)", fontsize=16)
        plt.tight_layout()
        plt.savefig("output/shap_summary_plot.png", dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… SHAP å…¨å±€ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: output/shap_summary_plot.png")

        # 2. å•æ ·æœ¬ç€‘å¸ƒå›¾ï¼ˆç¬¬0ä¸ªæ ·æœ¬ï¼‰
        if len(shap_values) > 0:
            plt.figure(figsize=(12, 8))
            shap.waterfall_plot(
                shap.Explanation(
                    values=shap_values[0],
                    base_values=explainer.expected_value,
                    data=x_val_sample.iloc[0],
                    feature_names=x_val_sample.columns.tolist()
                ),
                show=False
            )
            plt.title("SHAP Waterfall Plot - Sample 0", fontsize=16)
            plt.tight_layout()
            plt.savefig("output/shap_waterfall_sample_0.png", dpi=150, bbox_inches='tight')
            plt.close()
            print("âœ… SHAP å•æ ·æœ¬ç€‘å¸ƒå›¾å·²ä¿å­˜: output/shap_waterfall_sample_0.png")

    except Exception as e:
        print("âš ï¸ SHAP è§£é‡Šå¤±è´¥ï¼ˆè¯·ç¡®ä¿å·²å®‰è£… shapï¼‰:", e)

    # ========== Step 7: é¢„æµ‹æµ‹è¯•é›†å¹¶ä¿å­˜å¸¦ Id çš„ç»“æœ ==========
    y_pred = lr.predict_proba(test_lr)[:, 1]

    submission = pd.DataFrame({
        'Id': test_ids,
        'Probability': y_pred
    })

    # ========== Step 8: ä¿å­˜æ¨¡å‹å’Œå¿…è¦ä¿¡æ¯ç”¨äº API ==========
    joblib.dump(model, 'output/gbdt_model.pkl')
    joblib.dump(lr, 'output/lr_model.pkl')
    
    # ğŸ†• ä¿å­˜å®é™…æ ‘æ•°é‡ï¼Œä¾› API ä½¿ç”¨
    pd.Series([actual_n_estimators]).to_csv('output/actual_n_estimators.csv', index=False, header=['n_estimators'])
    
    pd.Series(x_train.columns).to_csv('output/train_feature_names.csv', index=False, header=['feature'])
    pd.Series(category_feature).to_csv('output/category_features.csv', index=False, header=['feature'])
    pd.Series(continuous_feature).to_csv('output/continuous_features.csv', index=False, header=['feature'])
    
    print("âœ… æ¨¡å‹å’Œå…ƒæ•°æ®å·²ä¿å­˜ï¼Œå¯ç”¨äº API æœåŠ¡")

    submission.to_csv('output/submission_gbdt_lr.csv', index=False)
    print("\nğŸ‰ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ output/submission_gbdt_lr.csvï¼ˆå« Id åˆ—ï¼‰")

    return y_pred


# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    data, test_ids = preProcess()

    # ========== ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾å®šä¹‰ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç‰¹å¾é…ç½®...")
    feature_config = pd.read_csv('config/features.csv')
    continuous_feature = feature_config[feature_config['feature_type'] == 'continuous']['feature_name'].tolist()
    category_feature = feature_config[feature_config['feature_type'] == 'category']['feature_name'].tolist()

    print("âœ… è¿ç»­ç‰¹å¾:", continuous_feature)
    print("âœ… ç±»åˆ«ç‰¹å¾:", category_feature)

    print("ğŸ§  å¼€å§‹è®­ç»ƒ GBDT + LR æ¨¡å‹...")
    predictions = gbdt_lr_predict(data, category_feature, continuous_feature, test_ids)

    print("\nâœ… æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹å®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - gbdt_feature_importance.csv")
    print("   - lr_leaf_coefficients.csv")
    print("   - shap_summary_plot.png")
    print("   - shap_waterfall_sample_0.png")
    print("   - submission_gbdt_lr.csv")
    print("   - actual_n_estimators.csv") 
