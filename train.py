import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
from lightgbm import log_evaluation
import matplotlib.pyplot as plt
import os
import joblib
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# ========== å·¥å…·å‡½æ•°ï¼šåŸºäº SHAP ä¾èµ–å…³ç³»è‡ªåŠ¨å»ºè®®åˆ†ç®±è¾¹ç•Œ ==========
def suggest_bins_from_shap(feature_values, shap_values, feature_name, n_bins=5):
    """
    åŸºäº SHAP ä¾èµ–å…³ç³»è‡ªåŠ¨å»ºè®®åˆ†ç®±è¾¹ç•Œ
    :param feature_values: è¯¥ç‰¹å¾çš„åŸå§‹å€¼ (1D array)
    :param shap_values: å¯¹åº”çš„ SHAP å€¼ (1D array)
    :param feature_name: ç‰¹å¾å
    :param n_bins: å°è¯•æ£€æµ‹çš„åˆ†ç®±æ•°ï¼ˆé»˜è®¤5ï¼‰
    :return: dict { "should_bin": bool, "suggested_bins": list, "reason": str }
    """
    try:
        # åˆå¹¶å¹¶æ’åº
        df = pd.DataFrame({
            'feature_val': feature_values,
            'shap_val': shap_values
        }).sort_values('feature_val').reset_index(drop=True)

        if len(df) < 10:
            return {"should_bin": False, "suggested_bins": [], "reason": "æ ·æœ¬å¤ªå°‘"}

        # æŒ‰ç‰¹å¾å€¼åˆ† n_bins ç»„ï¼ˆç­‰é¢‘ï¼‰
        df['group'] = pd.qcut(df['feature_val'], q=min(n_bins, len(df)//5), duplicates='drop')
        group_means = df.groupby('group', observed=False)['shap_val'].mean()
        group_bounds = df.groupby('group', observed=False)['feature_val'].agg(['min', 'max'])

        # æ£€æŸ¥ç›¸é‚»ç»„çš„ SHAP å‡å€¼å·®å¼‚
        mean_diffs = group_means.diff().abs().dropna()
        if len(mean_diffs) == 0:
            return {"should_bin": False, "suggested_bins": [], "reason": "æ— æ˜¾è‘—è·³è·ƒ"}

        # å¦‚æœæœ€å¤§è·³è·ƒ > å¹³å‡è·³è·ƒçš„ 1.5 å€ï¼Œè®¤ä¸ºæœ‰åˆ†ç®±ä»·å€¼
        avg_diff = mean_diffs.mean()
        max_diff = mean_diffs.max()
        if max_diff < avg_diff * 1.5:
            return {"should_bin": False, "suggested_bins": [], "reason": "SHAPå˜åŒ–å¹³ç¼“ï¼Œæ— éœ€åˆ†ç®±"}

        # å»ºè®®åˆ†ç®±è¾¹ç•Œï¼šå–æ¯ç»„æœ€å¤§å€¼ï¼ˆæœ€åä¸€ç»„é™¤å¤–ï¼‰
        suggested_bins = group_bounds['max'].iloc[:-1].tolist()
        # å››èˆäº”å…¥åˆ°åˆç†ç²¾åº¦ï¼ˆæ ¹æ®æ•°æ®èŒƒå›´ï¼‰
        if df['feature_val'].max() > 1000:
            suggested_bins = [round(b, -1) for b in suggested_bins]  # åä½å–æ•´
        elif df['feature_val'].max() > 100:
            suggested_bins = [round(b, 0) for b in suggested_bins]   # ä¸ªä½å–æ•´
        else:
            suggested_bins = [round(b, 2) for b in suggested_bins]   # ä¸¤ä½å°æ•°

        return {
            "should_bin": True,
            "suggested_bins": sorted(suggested_bins),
            "reason": f"æ£€æµ‹åˆ°æ˜¾è‘—SHAPè·³è·ƒï¼ˆæœ€å¤§è·³è·ƒ={max_diff:.3f} > å¹³å‡{avg_diff:.3f}ï¼‰"
        }

    except Exception as e:
        return {"should_bin": False, "suggested_bins": [], "reason": f"åˆ†æå¤±è´¥: {str(e)}"}


# ========== å·¥å…·å‡½æ•°ï¼šå»ºè®®æ˜¯å¦è¿›è¡Œå¯¹æ•°å˜æ¢ ==========
def suggest_log_transform(feature_values, shap_values, feature_name):
    """
    å»ºè®®æ˜¯å¦å¯¹ç‰¹å¾è¿›è¡Œå¯¹æ•°å˜æ¢
    æ¡ä»¶ï¼š
      - ç‰¹å¾å€¼å¿…é¡»å…¨ä¸ºæ­£æ•°
      - åˆ†å¸ƒå³åï¼ˆååº¦ > 1ï¼‰
      - SHAP å€¼éšç‰¹å¾å¢é•¿ä½†å¢é€Ÿæ”¾ç¼“ï¼ˆäºŒé˜¶å¯¼ä¸ºè´Ÿï¼‰
    """
    try:
        df = pd.DataFrame({
            'x': feature_values,
            'shap': shap_values
        }).sort_values('x').reset_index(drop=True)

        # å¿…é¡»å…¨ä¸ºæ­£æ•°
        if (df['x'] <= 0).any():
            return {"suggest_log": False, "reason": "åŒ…å«éæ­£å€¼ï¼Œæ— æ³•å–å¯¹æ•°", "code": ""}

        # è®¡ç®—ååº¦
        skewness = df['x'].skew()
        if skewness < 1.0:
            return {"suggest_log": False, "reason": f"ååº¦å¤ªä½ï¼ˆ{skewness:.2f}ï¼‰ï¼Œæ— éœ€å¯¹æ•°å˜æ¢", "code": ""}

        # æ£€æŸ¥ SHAP æ˜¯å¦å¢é€Ÿæ”¾ç¼“ï¼šè®¡ç®—ä¸€é˜¶å·®åˆ†çš„å·®åˆ†ï¼ˆè¿‘ä¼¼äºŒé˜¶å¯¼ï¼‰
        if len(df) < 5:
            return {"suggest_log": False, "reason": "æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿", "code": ""}

        df['shap_diff'] = df['shap'].diff()
        df['shap_diff2'] = df['shap_diff'].diff()

        # å–å 80% æ•°æ®ï¼ˆé¿å…å¼€å¤´å™ªå£°ï¼‰
        df_tail = df.iloc[int(len(df)*0.2):]
        mean_diff2 = df_tail['shap_diff2'].mean()

        # å¦‚æœäºŒé˜¶å¯¼ä¸ºè´Ÿï¼ˆå¢é€Ÿæ”¾ç¼“ï¼‰ï¼Œå»ºè®® log
        if mean_diff2 < -0.001:  # è´Ÿå€¼è¡¨ç¤ºå‡¹å‡½æ•°ï¼Œè¾¹é™…é€’å‡
            code = f"df['{feature_name}_log'] = np.log(df['{feature_name}'])"
            return {
                "suggest_log": True,
                "reason": f"å³ååˆ†å¸ƒï¼ˆååº¦={skewness:.2f}ï¼‰ä¸”è¾¹é™…æ•ˆåº”é€’å‡ï¼ˆäºŒé˜¶å¯¼å‡å€¼={mean_diff2:.4f}ï¼‰",
                "code": code
            }
        else:
            return {"suggest_log": False, "reason": f"æ— è¾¹é™…é€’å‡æ•ˆåº”ï¼ˆäºŒé˜¶å¯¼å‡å€¼={mean_diff2:.4f}ï¼‰", "code": ""}

    except Exception as e:
        return {"suggest_log": False, "reason": f"åˆ†æå¤±è´¥: {str(e)}", "code": ""}


# ========== å·¥å…·å‡½æ•°ï¼šå»ºè®®æ˜¯å¦æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ ==========
def suggest_polynomial(feature_values, shap_values, feature_name, max_degree=3):
    """
    å»ºè®®æ˜¯å¦æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ï¼ˆå¦‚ x^2, x^3ï¼‰
    æ–¹æ³•ï¼šæ‹Ÿåˆçº¿æ€§ã€äºŒæ¬¡ã€ä¸‰æ¬¡æ¨¡å‹ï¼Œçœ‹ RÂ² æå‡æ˜¯å¦æ˜¾è‘—ï¼ˆ>0.05ï¼‰
    """
    try:
        X = np.array(feature_values).reshape(-1, 1)
        y = np.array(shap_values)

        if len(X) < 10:
            return {"suggest_poly": False, "reason": "æ ·æœ¬å¤ªå°‘", "degree": None, "code": ""}

        # æ‹Ÿåˆçº¿æ€§æ¨¡å‹ä½œä¸º baseline
        lr_linear = LinearRegression()
        lr_linear.fit(X, y)
        r2_linear = r2_score(y, lr_linear.predict(X))

        best_degree = 1
        best_r2 = r2_linear
        best_improvement = 0

        for degree in range(2, max_degree + 1):
            poly = PolynomialFeatures(degree=degree, include_bias=False)
            X_poly = poly.fit_transform(X)
            lr = LinearRegression()
            lr.fit(X_poly, y)
            r2 = r2_score(y, lr.predict(X_poly))
            improvement = r2 - r2_linear

            if improvement > best_improvement and improvement > 0.05:  # è‡³å°‘æå‡ 0.05 RÂ²
                best_improvement = improvement
                best_degree = degree
                best_r2 = r2

        if best_degree > 1:
            code = f"df['{feature_name}_pow{best_degree}'] = df['{feature_name}'] ** {best_degree}"
            return {
                "suggest_poly": True,
                "degree": best_degree,
                "r2_improvement": best_improvement,
                "reason": f"æ£€æµ‹åˆ°éçº¿æ€§æ¨¡å¼ï¼Œ{best_degree}æ¬¡å¤šé¡¹å¼å¯æå‡è§£é‡ŠåŠ›ï¼ˆRÂ²æå‡={best_improvement:.3f}ï¼‰",
                "code": code
            }
        else:
            return {
                "suggest_poly": False,
                "reason": f"æ— çº¿æ€§å¤–æ˜¾è‘—æ¨¡å¼ï¼ˆæœ€å¤§RÂ²æå‡={best_improvement:.3f}ï¼‰",
                "degree": None,
                "code": ""
            }

    except Exception as e:
        return {"suggest_poly": False, "reason": f"åˆ†æå¤±è´¥: {str(e)}", "degree": None, "code": ""}


# ========== å·¥å…·å‡½æ•°ï¼šè§£æå¶å­èŠ‚ç‚¹è·¯å¾„ï¼ˆå¢å¼ºç‰ˆï¼‰ ==========
def get_leaf_path_enhanced(booster, tree_index, leaf_index, feature_names, category_prefixes):
    """
    è§£ææŒ‡å®šå¶å­èŠ‚ç‚¹çš„å†³ç­–è·¯å¾„ï¼Œæ”¯æŒç¿»è¯‘ one-hot ç±»åˆ«ç‰¹å¾
    """
    try:
        model_dump = booster.dump_model()
        if tree_index >= len(model_dump['tree_info']):
            return None
        tree_info = model_dump['tree_info'][tree_index]['tree_structure']
    except Exception as e:
        print(f"è·å–æ ‘ç»“æ„å¤±è´¥: {e}")
        return None

    node_stack = [(tree_info, [])]  # (å½“å‰èŠ‚ç‚¹, è·¯å¾„åˆ—è¡¨)

    while node_stack:
        node, current_path = node_stack.pop()

        # å¦‚æœæ˜¯ç›®æ ‡å¶å­èŠ‚ç‚¹
        if 'leaf_index' in node and node['leaf_index'] == leaf_index:
            return current_path

        # å¦‚æœæ˜¯åˆ†è£‚èŠ‚ç‚¹
        if 'split_feature' in node:
            feat_idx = node['split_feature']
            if feat_idx >= len(feature_names):
                feat_name = f"Feature_{feat_idx}"
            else:
                feat_name = feature_names[feat_idx]

            threshold = node.get('threshold', 0.0)
            decision_type = node.get('decision_type', '<=')

            # æ£€æŸ¥æ˜¯å¦ä¸º one-hot ç±»åˆ«ç‰¹å¾
            is_category = False
            original_col = None
            category_value = None

            for prefix in category_prefixes:
                if feat_name.startswith(prefix):
                    is_category = True
                    original_col = prefix.rstrip('_')
                    category_value = feat_name[len(prefix):]
                    break

            if is_category:
                # ç±»åˆ«ç‰¹å¾é€šå¸¸ç”¨ > 0.5 åˆ¤æ–­æ˜¯å¦æ¿€æ´»
                # å‡è®¾å³å­æ ‘æ˜¯â€œç­‰äºè¯¥ç±»åˆ«â€
                right_rule = f"{original_col} == '{category_value}'"
                left_rule = f"{original_col} != '{category_value}'"
            else:
                # è¿ç»­ç‰¹å¾
                if decision_type == '<=' or decision_type == 'no_greater':
                    right_rule = f"{feat_name} > {threshold:.4f}"
                    left_rule = f"{feat_name} <= {threshold:.4f}"
                else:
                    right_rule = f"{feat_name} {decision_type} {threshold:.4f}"
                    left_rule = f"{feat_name} not {decision_type} {threshold:.4f}"

            # æ·»åŠ å·¦å³å­æ ‘åˆ°æ ˆ
            if 'right_child' in node:
                node_stack.append((node['right_child'], current_path + [right_rule]))
            if 'left_child' in node:
                node_stack.append((node['left_child'], current_path + [left_rule]))

    return None  # æœªæ‰¾åˆ°è·¯å¾„


# ========== æ•°æ®é¢„å¤„ç† ==========
def preProcess():
    path = 'data/'
    df_train = pd.read_csv(path + 'train.csv')
    df_test = pd.read_csv(path + 'test.csv')
    
    test_ids = df_test['Id'].copy()
    
    df_train.drop(['Id'], axis=1, inplace=True)
    df_test.drop(['Id'], axis=1, inplace=True)
    
    df_test['Label'] = -1
    data = pd.concat([df_train, df_test], ignore_index=True)
    data = data.fillna(-1)
    
    data.to_csv('data/data.csv', index=False)
    
    return data, test_ids


# ========== GBDT + LR æ ¸å¿ƒè®­ç»ƒé¢„æµ‹å‡½æ•° ==========
def gbdt_lr_predict(data, category_feature, continuous_feature, test_ids):
    """
    ä½¿ç”¨ GBDT + LRï¼Œå¢å¼ºå¯è§£é‡Šæ€§è¾“å‡º
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('output', exist_ok=True)

    # ========== Step 1: ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç  ==========
    for col in category_feature:
        if data[col].dtype == 'object':
            data[col] = data[col].astype('category')
        onehot_feats = pd.get_dummies(data[col], prefix=col)
        data.drop([col], axis=1, inplace=True)
        data = pd.concat([data, onehot_feats], axis=1)

    # ========== ğŸ†• Step 1.5: åº”ç”¨æ™ºèƒ½ç‰¹å¾å·¥ç¨‹å»ºè®® ==========
    # æ‰‹åŠ¨æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ï¼ˆæ ¹æ®ä¹‹å‰å»ºè®®ï¼‰
    #data['I5_pow3'] = data['I5'] ** 3
    #data['I11_pow3'] = data['I11'] ** 3
    #data['I13_pow3'] = data['I13'] ** 3
    #data['I6_pow3'] = data['I6'] ** 3
    #data['I3_pow3'] = data['I3'] ** 3
    #data['I8_pow3'] = data['I8'] ** 3
    #data['I2_pow3'] = data['I2'] ** 3
    #pd.cut(data['I5'], bins=[-np.inf, 140.0, 1020.0, 2960.0, 14410.0, np.inf])

    train = data[data['Label'] != -1].copy()
    target = train.pop('Label')
    test = data[data['Label'] == -1].copy()
    test.drop(['Label'], axis=1, inplace=True)

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    x_train, x_val, y_train, y_val = train_test_split(
        train, target, test_size=0.2, random_state=2020, stratify=target
    )

    # ========== Step 2: è®­ç»ƒ GBDT ==========
    n_estimators = 32
    num_leaves = 64

    model = lgb.LGBMClassifier(
        objective='binary',
        boosting_type='gbdt',
        subsample=0.8,
        min_child_weight=0.5,
        colsample_bytree=0.7,
        num_leaves=num_leaves,
        learning_rate=0.05,
        n_estimators=n_estimators,
        random_state=2020,
        n_jobs=-1
    )

    model.fit(
        x_train, y_train,
        eval_set=[(x_train, y_train), (x_val, y_val)],
        eval_metric='binary_logloss',
        callbacks=[
            log_evaluation(0),
            lgb.early_stopping(stopping_rounds=5, verbose=False)
        ]
    )

    # ========== ğŸ†• è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡ï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰ ==========
    actual_n_estimators = model.best_iteration_
    print(f"âœ… å®é™…è®­ç»ƒæ ‘æ•°é‡: {actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")

    # ========== Step 2.5: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ ==========
    feat_imp = pd.DataFrame({
        'ç‰¹å¾å': x_train.columns,
        'é‡è¦æ€§': model.feature_importances_
    }).sort_values('é‡è¦æ€§', ascending=False)

    print("\n" + "="*60)
    print("ğŸ“Š GBDT å‰20é‡è¦ç‰¹å¾:")
    print("="*60)
    print(feat_imp.head(20))
    feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False, encoding='utf-8-sig')
    print("âœ… å·²ä¿å­˜è‡³ output/gbdt_feature_importance.csv")

    # ========== Step 3: è·å–å¶å­èŠ‚ç‚¹ç´¢å¼• ==========
    gbdt_feats_train = model.booster_.predict(train.values, pred_leaf=True)
    gbdt_feats_test = model.booster_.predict(test.values, pred_leaf=True)

    print("\nâœ… GBDT å¶å­èŠ‚ç‚¹ç´¢å¼• shape:", gbdt_feats_train.shape)
    print("âœ… å‰5ä¸ªæ ·æœ¬å¶å­ç´¢å¼•:\n", gbdt_feats_train[:5])

    # ========== Step 4: å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç  ==========
    # ğŸ†• ä½¿ç”¨ actual_n_estimators æ›¿ä»£ç¡¬ç¼–ç  n_estimators
    gbdt_feats_name = ['gbdt_leaf_' + str(i) for i in range(actual_n_estimators)]

    df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train, columns=gbdt_feats_name)
    df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test, columns=gbdt_feats_name)

    train_len = df_train_gbdt_feats.shape[0]
    data_gbdt = pd.concat([df_train_gbdt_feats, df_test_gbdt_feats], ignore_index=True)

    for col in gbdt_feats_name:
        onehot_feats = pd.get_dummies(data_gbdt[col], prefix=col)
        data_gbdt.drop([col], axis=1, inplace=True)
        data_gbdt = pd.concat([data_gbdt, onehot_feats], axis=1)

    train_lr = data_gbdt.iloc[:train_len, :].reset_index(drop=True)
    test_lr = data_gbdt.iloc[train_len:, :].reset_index(drop=True)

    # ========== Step 5: è®­ç»ƒ LR æ¨¡å‹ ==========
    x_train_lr, x_val_lr, y_train_lr, y_val_lr = train_test_split(
        train_lr, target, test_size=0.3, random_state=2018, stratify=target
    )

    lr = LogisticRegression(
        penalty='l2',
        C=0.1,
        solver='liblinear',
        random_state=2018,
        max_iter=1000
    )
    lr.fit(x_train_lr, y_train_lr)

    tr_logloss = log_loss(y_train_lr, lr.predict_proba(x_train_lr)[:, 1])
    val_logloss = log_loss(y_val_lr, lr.predict_proba(x_val_lr)[:, 1])
    print('\nâœ… è®­ç»ƒé›† LogLoss:', tr_logloss)
    print('âœ… éªŒè¯é›† LogLoss:', val_logloss)

    # ========== Step 5.5: è¾“å‡º LR ç³»æ•°ï¼ˆå“ªäº›å¶å­è§„åˆ™æœ€é‡è¦ï¼‰ ==========
    lr_coef = pd.DataFrame({
        'å¶å­ç‰¹å¾': x_train_lr.columns,
        'ç³»æ•°': lr.coef_[0]
    }).sort_values('ç³»æ•°', key=abs, ascending=False)

    print("\n" + "="*60)
    print("ğŸ“Š LR å‰20é‡è¦å¶å­ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºï¼‰:")
    print("="*60)
    print(lr_coef.head(20))
    lr_coef.to_csv('output/lr_leaf_coefficients.csv', index=False, encoding='utf-8-sig')
    print("âœ… å·²ä¿å­˜è‡³ output/lr_leaf_coefficients.csv")

    # ========== Step 5.6: å¯¹é«˜æƒé‡å¶å­è¿›è¡Œè§„åˆ™è§£æ ==========
    print("\n" + "="*70)
    print("ğŸ§  è§£æ LR ä¸­é«˜æƒé‡å¶å­èŠ‚ç‚¹å¯¹åº”çš„åŸå§‹è§„åˆ™")
    print("="*70)
    
    top_leaves = lr_coef.head(5)  # è§£æå‰5ä¸ªæœ€é‡è¦å¶å­
    category_prefixes = [col + "_" for col in category_feature]
    
    for idx, row in top_leaves.iterrows():
        leaf_feat = row['å¶å­ç‰¹å¾']
        coef = row['ç³»æ•°']
        
        # è§£æå¶å­åç§°ï¼Œå¦‚ "gbdt_leaf_5_22"
        if leaf_feat.startswith('gbdt_leaf_'):
            parts = leaf_feat.split('_')
            if len(parts) >= 4:
                tree_idx = int(parts[2])
                leaf_idx = int(parts[3])
                
                print(f"\nğŸ” è§£æ {leaf_feat} (LRç³»æ•°: {coef:.4f})")
                try:
                    rule = get_leaf_path_enhanced(
                        model.booster_,
                        tree_index=tree_idx,
                        leaf_index=leaf_idx,
                        feature_names=x_train.columns.tolist(),
                        category_prefixes=category_prefixes
                    )
                    if rule:
                        for i, r in enumerate(rule, 1):
                            print(f"   {i}. {r}")
                    else:
                        print("   âš ï¸ è·¯å¾„æœªæ‰¾åˆ°")
                except Exception as e:
                    print(f"   âš ï¸ è§£æå¤±è´¥: {e}")

    # ========== Step 6: SHAP è§£é‡Šï¼ˆå…¨å±€ + å•æ ·æœ¬ï¼‰ ==========
    print("\n" + "="*60)
    print("ğŸ¨ æ­£åœ¨ç”Ÿæˆ SHAP å¯è§£é‡Šæ€§å›¾è¡¨...")
    print("="*60)
    
    try:
        import shap
        explainer = shap.TreeExplainer(model.booster_)
        
        sample_size = min(100, len(x_val))
        x_val_sample = x_val.iloc[:sample_size]
        shap_values = explainer.shap_values(x_val_sample)

        # 1. å…¨å±€ç‰¹å¾é‡è¦æ€§å›¾
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, x_val_sample, feature_names=x_val_sample.columns.tolist(), show=False)
        plt.title("SHAP Feature Importance (GBDT)", fontsize=16)
        plt.tight_layout()
        plt.savefig("output/shap_summary_plot.png", dpi=150, bbox_inches='tight')
        plt.close()
        print("âœ… SHAP å…¨å±€ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: output/shap_summary_plot.png")

        # 2. å•æ ·æœ¬ç€‘å¸ƒå›¾ï¼ˆç¬¬0ä¸ªæ ·æœ¬ï¼‰
        if len(shap_values) > 0:
            plt.figure(figsize=(12, 8))
            shap.waterfall_plot(
                shap.Explanation(
                    values=shap_values[0],
                    base_values=explainer.expected_value,
                    data=x_val_sample.iloc[0],
                    feature_names=x_val_sample.columns.tolist()
                ),
                show=False
            )
            plt.title("SHAP Waterfall Plot - Sample 0", fontsize=16)
            plt.tight_layout()
            plt.savefig("output/shap_waterfall_sample_0.png", dpi=150, bbox_inches='tight')
            plt.close()
            print("âœ… SHAP å•æ ·æœ¬ç€‘å¸ƒå›¾å·²ä¿å­˜: output/shap_waterfall_sample_0.png")

        # ========== Step 6.5: ç”Ÿæˆ SHAP Dependence Plots + ç‰¹å¾å·¥ç¨‹å»ºè®® ==========
        print("\n" + "="*60)
        print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆ SHAP ä¾èµ–å›¾å’Œç‰¹å¾å·¥ç¨‹å»ºè®®...")
        print("="*60)

        all_suggestions = []  # æ”¶é›†æ‰€æœ‰å»ºè®®ï¼ˆåˆ†ç®±ã€logã€å¤šé¡¹å¼ï¼‰

        try:
            # è·å–ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆç”¨äºé€‰æ‹© Top ç‰¹å¾ï¼‰
            if isinstance(shap_values, list):
                shap_vals_for_imp = shap_values[1]  # äºŒåˆ†ç±»å–æ­£ç±»
            else:
                shap_vals_for_imp = shap_values

            # è®¡ç®—å¹³å‡ç»å¯¹ SHAP å€¼ä½œä¸ºé‡è¦æ€§
            feature_importance = np.abs(shap_vals_for_imp).mean(axis=0)
            top_feature_indices = np.argsort(feature_importance)[::-1][:10]  # Top 10

            for idx in top_feature_indices:
                feature_name = x_val_sample.columns[idx]
                safe_feature_name = "".join(c if c.isalnum() or c in "._-" else "_" for c in feature_name)

                # ========== ğŸ†• ä»…ä¿®æ”¹ä»¥ä¸‹ç»˜å›¾å— ==========
                plt.figure(figsize=(10, 6))
                shap.dependence_plot(
                    idx,
                    shap_vals_for_imp,
                    x_val_sample,
                    feature_names=x_val_sample.columns.tolist(),
                    show=False
                )
                plt.title(f"SHAP Dependence Plot: {feature_name}", fontsize=14)  # âœ… æ”¹ä¸ºè‹±æ–‡
                plt.xlabel(f"{feature_name} (Value)")                           # âœ… ç¡®ä¿è‹±æ–‡
                plt.ylabel("SHAP Value (Contribution to Prediction)")           # âœ… ç¡®ä¿è‹±æ–‡
                plt.xticks(rotation=45)
                plt.tight_layout()
                # ========== ä¿®æ”¹ç»“æŸ ==========

                plot_filename = f"shap_dependence_{safe_feature_name}.png"
                plot_path = os.path.join("output", plot_filename)
                plt.savefig(plot_path, dpi=150, bbox_inches='tight')
                plt.close()
                print(f"\nâœ… å·²ä¿å­˜ä¾èµ–å›¾: {plot_filename}")

                feat_vals = x_val_sample.iloc[:, idx].values
                shap_vals = shap_vals_for_imp[:, idx]

                # ========== ğŸ†• åˆ†ç®±å»ºè®® ==========
                bin_suggestion = suggest_bins_from_shap(feat_vals, shap_vals, feature_name)
                if bin_suggestion["should_bin"] and len(bin_suggestion["suggested_bins"]) >= 2:
                    bins_str = ", ".join(map(str, bin_suggestion["suggested_bins"]))
                    code = f"pd.cut(df['{feature_name}'], bins=[-np.inf, {bins_str}, np.inf])"
                    print(f"    ğŸ§  [åˆ†ç®±å»ºè®®] {code}")
                    print(f"        ç†ç”±: {bin_suggestion['reason']}")
                    all_suggestions.append({
                        "ç‰¹å¾å": feature_name,
                        "å»ºè®®ç±»å‹": "åˆ†ç®±",
                        "ä»£ç ": code,
                        "ç†ç”±": bin_suggestion["reason"],
                        "å¯¹åº”å›¾è¡¨": plot_filename
                    })

                # ========== ğŸ†• Log å˜æ¢å»ºè®® ==========
                log_suggestion = suggest_log_transform(feat_vals, shap_vals, feature_name)
                if log_suggestion["suggest_log"]:
                    print(f"    ğŸ“Š [å¯¹æ•°å˜æ¢å»ºè®®] {log_suggestion['code']}")
                    print(f"        ç†ç”±: {log_suggestion['reason']}")
                    all_suggestions.append({
                        "ç‰¹å¾å": feature_name,
                        "å»ºè®®ç±»å‹": "å¯¹æ•°å˜æ¢",
                        "ä»£ç ": log_suggestion["code"],
                        "ç†ç”±": log_suggestion["reason"],
                        "å¯¹åº”å›¾è¡¨": plot_filename
                    })

                # ========== ğŸ†• å¤šé¡¹å¼ç‰¹å¾å»ºè®® ==========
                poly_suggestion = suggest_polynomial(feat_vals, shap_vals, feature_name)
                if poly_suggestion["suggest_poly"]:
                    print(f"    ğŸ“ˆ [å¤šé¡¹å¼å»ºè®®] {poly_suggestion['code']}")
                    print(f"        ç†ç”±: {poly_suggestion['reason']}")
                    all_suggestions.append({
                        "ç‰¹å¾å": feature_name,
                        "å»ºè®®ç±»å‹": f"å¤šé¡¹å¼_{poly_suggestion['degree']}æ¬¡",
                        "ä»£ç ": poly_suggestion["code"],
                        "ç†ç”±": poly_suggestion["reason"],
                        "å¯¹åº”å›¾è¡¨": plot_filename
                    })

            # ä¿å­˜æ‰€æœ‰å»ºè®®åˆ° CSV
            if all_suggestions:
                suggestion_df = pd.DataFrame(all_suggestions)
                suggestion_df.to_csv("output/feature_engineering_suggestions.csv", index=False, encoding='utf-8-sig')
                print(f"\nâœ… æ‰€æœ‰ç‰¹å¾å·¥ç¨‹å»ºè®®å·²ä¿å­˜è‡³: output/feature_engineering_suggestions.csv")

            # æ‰“å°ä½¿ç”¨æŒ‡å—ï¼ˆä¸­æ–‡ä¿ç•™ï¼Œä¸å½±å“ç»˜å›¾ï¼‰
            print("\nğŸ“˜ SHAP ä¾èµ–å›¾è§£è¯»ä¸å»ºè®®æŒ‡å—:")
            print("  - â†— ç›´çº¿ä¸Šå‡ â†’ çº¿æ€§æ­£ç›¸å…³ â†’ æ— éœ€å˜æ¢")
            print("  - â†˜ ç›´çº¿ä¸‹é™ â†’ çº¿æ€§è´Ÿç›¸å…³ â†’ æ— éœ€å˜æ¢")
            print("  - âŒ£ Uå‹æ›²çº¿ â†’ éçº¿æ€§ â†’ å¯å°è¯•åŠ å¹³æ–¹é¡¹")
            print("  - âŒ¢ å€’Uå‹ â†’ è¾¹é™…æ•ˆåº”é€’å‡ â†’ å¯å°è¯• log æˆ–åˆ†ç®±")
            print("  - â–‚â–ƒâ–…â–‡ é˜¶æ¢¯çŠ¶ â†’ åˆ†æ®µæ•ˆåº” â†’ å»ºè®®åˆ†ç®±")
            print("  - â° å³ååˆ†å¸ƒ + è¾¹é™…é€’å‡ â†’ å»ºè®® log(x)")
            print("  - ğŸ’¡ ä¸Šè¿°å»ºè®®å·²è¾“å‡ºåˆ°æ§åˆ¶å°å’Œ CSV æ–‡ä»¶")

        except Exception as e:
            print(f"âš ï¸ SHAP ä¾èµ–å›¾æˆ–å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")

    except Exception as e:
        print("âš ï¸ SHAP è§£é‡Šå¤±è´¥ï¼ˆè¯·ç¡®ä¿å·²å®‰è£… shapï¼‰:", e)

    # ========== Step 7: é¢„æµ‹æµ‹è¯•é›†å¹¶ä¿å­˜å¸¦ Id çš„ç»“æœ ==========
    y_pred = lr.predict_proba(test_lr)[:, 1]

    submission = pd.DataFrame({
        'Id': test_ids,
        'Probability': y_pred
    })

    # ========== Step 8: ä¿å­˜æ¨¡å‹å’Œå¿…è¦ä¿¡æ¯ç”¨äº API ==========
    joblib.dump(model, 'output/gbdt_model.pkl')
    joblib.dump(lr, 'output/lr_model.pkl')
    # ğŸ†• ä¿å­˜å®é™…æ ‘æ•°é‡
    pd.Series([actual_n_estimators]).to_csv('output/actual_n_estimators.csv', index=False, header=['n_estimators'])
    pd.Series(x_train.columns).to_csv('output/train_feature_names.csv', index=False, header=['ç‰¹å¾å'], encoding='utf-8-sig')
    pd.Series(category_feature).to_csv('output/category_features.csv', index=False, header=['ç‰¹å¾å'], encoding='utf-8-sig')
    pd.Series(continuous_feature).to_csv('output/continuous_features.csv', index=False, header=['ç‰¹å¾å'], encoding='utf-8-sig')
    
    print("âœ… æ¨¡å‹å’Œå…ƒæ•°æ®å·²ä¿å­˜ï¼Œå¯ç”¨äº API æœåŠ¡")

    submission.to_csv('output/submission_gbdt_lr.csv', index=False, encoding='utf-8-sig')
    print("\nğŸ‰ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³ output/submission_gbdt_lr.csvï¼ˆå« Id åˆ—ï¼‰")

    return y_pred


# ========== ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
    data, test_ids = preProcess()

    # ========== ä»é…ç½®æ–‡ä»¶è¯»å–ç‰¹å¾å®šä¹‰ ==========
    print("ğŸ“‚ æ­£åœ¨åŠ è½½ç‰¹å¾é…ç½®...")
    feature_config = pd.read_csv('config/features.csv')
    continuous_feature = feature_config[feature_config['feature_type'] == 'continuous']['feature_name'].tolist()
    category_feature = feature_config[feature_config['feature_type'] == 'category']['feature_name'].tolist()

    print("âœ… è¿ç»­ç‰¹å¾:", continuous_feature)
    print("âœ… ç±»åˆ«ç‰¹å¾:", category_feature)

    print("ğŸ§  å¼€å§‹è®­ç»ƒ GBDT + LR æ¨¡å‹...")
    predictions = gbdt_lr_predict(data, category_feature, continuous_feature, test_ids)

    print("\nâœ… æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹å®Œæˆï¼")
    print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
    print("   - gbdt_feature_importance.csv")
    print("   - lr_leaf_coefficients.csv")
    print("   - shap_summary_plot.png")
    print("   - shap_waterfall_sample_0.png")
    print("   - shap_dependence_*.png")
    print("   - feature_engineering_suggestions.csv â† ğŸ†• æ–°å¢ï¼ç‰¹å¾å·¥ç¨‹æ™ºèƒ½å»ºè®®")
    print("   - submission_gbdt_lr.csv")
    print("   - æ§åˆ¶å°è¾“å‡ºå¶å­èŠ‚ç‚¹è§„åˆ™è§£æ")
